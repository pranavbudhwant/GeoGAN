{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Text, Dict, Any, Iterator\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(in_shape: Tuple[int,int,int], out_shape: Tuple[int,int,int], filters: int):\n",
    "    \n",
    "    input_tensor = Input(shape = in_shape)\n",
    "    noise = Input(shape = (in_shape[0], in_shape[1], 1))\n",
    "    output_ch = out_shape[2]\n",
    "        \n",
    "    #Encoder\n",
    "    x = BatchNormalization()(Conv2D(filters*1, kernel_size = (5, 5), strides = (2, 2), padding = \"same\")(Concatenate()([input_tensor, noise])))\n",
    "    x = LeakyReLU(0.2)(x); e1 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*2, kernel_size = (5, 5), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e2 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*4, kernel_size = (5, 5), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e3 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*8, kernel_size = (5, 5), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e4 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e5 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e6 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e7 = x\n",
    "    x = BatchNormalization()(Conv2D(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); e8 = x\n",
    "    \n",
    "    #Decoder\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e7])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e6])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e5])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*8, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e4])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*4, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e3])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*2, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e2])\n",
    "    x = BatchNormalization()(Conv2DTranspose(filters*1, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x))\n",
    "    x = LeakyReLU(0.2)(x); x = Concatenate()([Dropout(0.5)(x), e1])\n",
    "    x = Conv2DTranspose(output_ch, kernel_size = (4, 4), strides = (2, 2), padding = \"same\")(x)\n",
    "    x = Activation(\"tanh\")(x)\n",
    "    \n",
    "    unet = Model(inputs=[input_tensor, noise], outputs = [x])\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 512, 512, 11) 0           input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 256, 256, 64) 17664       concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 256, 256, 64) 256         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 256, 256, 64) 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 128, 128, 128 204928      leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 128, 128, 128 512         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 128, 128, 128 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 256)  819456      leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 64, 64, 256)  1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 64, 64, 256)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 512)  3277312     leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 32, 32, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 32, 32, 512)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 512)  4194816     leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 16, 16, 512)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 8, 8, 512)    4194816     leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 512)    2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 8, 8, 512)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 512)    4194816     leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 4, 512)    2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 4, 4, 512)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 2, 2, 512)    4194816     leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 2, 2, 512)    2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 2, 2, 512)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_33 (Conv2DTran (None, 4, 4, 512)    4194816     leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 4, 512)    2048        conv2d_transpose_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 4, 4, 512)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 4, 4, 512)    0           leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 1024)   0           dropout_29[0][0]                 \n",
      "                                                                 leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_34 (Conv2DTran (None, 8, 8, 512)    8389120     concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 512)    2048        conv2d_transpose_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 8, 8, 512)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 512)    0           leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 8, 8, 1024)   0           dropout_30[0][0]                 \n",
      "                                                                 leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_35 (Conv2DTran (None, 16, 16, 512)  8389120     concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 512)  2048        conv2d_transpose_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)      (None, 16, 16, 512)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 16, 16, 512)  0           leaky_re_lu_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 16, 16, 1024) 0           dropout_31[0][0]                 \n",
      "                                                                 leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_36 (Conv2DTran (None, 32, 32, 512)  8389120     concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 32, 32, 512)  2048        conv2d_transpose_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)      (None, 32, 32, 512)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 32, 32, 512)  0           leaky_re_lu_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 32, 32, 1024) 0           dropout_32[0][0]                 \n",
      "                                                                 leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_37 (Conv2DTran (None, 64, 64, 256)  4194560     concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 64, 64, 256)  1024        conv2d_transpose_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)      (None, 64, 64, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 64, 64, 256)  0           leaky_re_lu_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 64, 64, 512)  0           dropout_33[0][0]                 \n",
      "                                                                 leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_38 (Conv2DTran (None, 128, 128, 128 1048704     concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 128, 128, 128 512         conv2d_transpose_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)      (None, 128, 128, 128 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 128, 128, 128 0           leaky_re_lu_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 128, 128, 256 0           dropout_34[0][0]                 \n",
      "                                                                 leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_39 (Conv2DTran (None, 256, 256, 64) 262208      concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 256, 256, 64) 256         conv2d_transpose_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)      (None, 256, 256, 64) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 256, 256, 64) 0           leaky_re_lu_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 256, 256, 128 0           dropout_35[0][0]                 \n",
      "                                                                 leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_40 (Conv2DTran (None, 512, 512, 3)  6147        concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 512, 512, 3)  0           conv2d_transpose_40[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 55,994,435\n",
      "Trainable params: 55,983,427\n",
      "Non-trainable params: 11,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net = generator((512, 512, 10), (512,512,3), 64)\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(in_shape: Tuple[int,int,int], y_shape: Tuple[int,int,int], filters: int):\n",
    "    input_tensor = Input(shape=in_shape)\n",
    "    input_y = Input(shape=y_shape)\n",
    "    x = LeakyReLU(0.2)(Conv2D(filters*1, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(Concatenate()([input_tensor, input_y])))\n",
    "    x = LeakyReLU(0.2)(BatchNormalization()(Conv2D(filters*2, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)))\n",
    "    x = LeakyReLU(0.2)(BatchNormalization()(Conv2D(filters*4, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)))\n",
    "    x = LeakyReLU(0.2)(BatchNormalization()(Conv2D(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)))\n",
    "    x = LeakyReLU(0.2)(BatchNormalization()(Conv2D(filters*8, kernel_size=(4, 4), strides=(1, 1), padding=\"same\")(x)))\n",
    "    x = LeakyReLU(0.2)(BatchNormalization()(Conv2D(filters*4, kernel_size=(4, 4), strides=(1, 1), padding=\"same\")(x)))\n",
    "    x = Activation(\"sigmoid\")(Conv2D(1, kernel_size=(4, 4), strides=(1, 1), padding=\"same\")(x))\n",
    "\n",
    "    disc = Model(inputs=[input_tensor, input_y], outputs=[x])\n",
    "    return disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 512, 512, 13) 0           input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 256, 256, 64) 13376       concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)     (None, 256, 256, 64) 0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 128, 128, 128 131200      leaky_re_lu_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 128, 128, 128 512         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)     (None, 128, 128, 128 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 256)  524544      leaky_re_lu_101[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 64, 64, 256)  1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)     (None, 64, 64, 256)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 512)  2097664     leaky_re_lu_102[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 32, 32, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_103 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 512)  4194816     leaky_re_lu_103[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 32, 32, 512)  2048        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_104 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 256)  2097408     leaky_re_lu_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 32, 32, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_105 (LeakyReLU)     (None, 32, 32, 256)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 1)    4097        leaky_re_lu_105[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 1)    0           conv2d_75[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,069,761\n",
      "Trainable params: 9,066,433\n",
      "Non-trainable params: 3,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = discriminator((512, 512, 10), (512, 512, 3), 64)\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cGAN_Model(gen, disc):\n",
    "    input_gen = Input((512, 512, 10))\n",
    "    noise = Input((512, 512, 1))\n",
    "    output_gen = gen([input_gen, noise])\n",
    "    #input_disc = Concatenate()([input_gen, output_gen])\n",
    "    disc.trainable = False\n",
    "    output_disc = disc([input_gen, output_gen])\n",
    "    cGAN = Model(inputs = [input_gen, noise], outputs = [output_gen, output_disc])\n",
    "    return cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Model)                (None, 512, 512, 3)  55994435    input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_14 (Model)                (None, 32, 32, 1)    9069761     input_31[0][0]                   \n",
      "                                                                 model_13[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 65,064,196\n",
      "Trainable params: 55,983,427\n",
      "Non-trainable params: 9,080,769\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cgan = cGAN_Model(net, disc)\n",
    "cgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(in_shape: Tuple[int,int,int]):\n",
    "    return np.random.normal(0, 1, size=in_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.00298566]\n",
      "  [ 0.92127517]\n",
      "  [ 0.1392467 ]\n",
      "  [ 2.10072708]\n",
      "  [-0.22242183]]\n",
      "\n",
      " [[-0.86809901]\n",
      "  [-2.06512589]\n",
      "  [ 1.17634411]\n",
      "  [-0.78937999]\n",
      "  [-0.33870604]]\n",
      "\n",
      " [[-0.37542468]\n",
      "  [-0.3722606 ]\n",
      "  [-2.96793816]\n",
      "  [-0.19585321]\n",
      "  [ 0.33705055]]\n",
      "\n",
      " [[ 1.46118661]\n",
      "  [-0.59830543]\n",
      "  [ 0.35474009]\n",
      "  [ 1.68559424]\n",
      "  [-0.28919315]]\n",
      "\n",
      " [[ 1.3459891 ]\n",
      "  [-0.01493187]\n",
      "  [ 0.83713128]\n",
      "  [ 1.05288414]\n",
      "  [-0.24854697]]]\n"
     ]
    }
   ],
   "source": [
    "print(generate_noise((5, 5, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(y_true, y_pred): #MAE\n",
    "    return  tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def discriminator_on_generator_loss(y_true, y_pred): #Binary Cross Entropy\n",
    "    BATCH_SIZE=10\n",
    "    \n",
    "    #K.mean(K.binary_crossentropy(K.flatten(y_pred), K.flatten(y_true)), axis = -1)\n",
    "    \n",
    "    return K.mean(K.binary_crossentropy(K.flatten(y_pred), K.ones_like(K.flatten(y_pred))), axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "cgan_optimizer = Adam(lr=1E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "gen_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.999) \n",
    "loss_weights = [10, 1]\n",
    "disc_optimizer = Adam(lr=1E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Summary\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 512, 512, 11) 0           input_33[0][0]                   \n",
      "                                                                 input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 256, 256, 64) 17664       concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 256, 256, 64) 256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_106 (LeakyReLU)     (None, 256, 256, 64) 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 128, 128, 128 204928      leaky_re_lu_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 128, 128, 128 512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_107 (LeakyReLU)     (None, 128, 128, 128 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 64, 64, 256)  819456      leaky_re_lu_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 64, 64, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_108 (LeakyReLU)     (None, 64, 64, 256)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 512)  3277312     leaky_re_lu_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 32, 32, 512)  2048        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_109 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 512)  4194816     leaky_re_lu_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 512)  2048        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_110 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 512)    4194816     leaky_re_lu_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 8, 8, 512)    2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_111 (LeakyReLU)     (None, 8, 8, 512)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 512)    4194816     leaky_re_lu_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 4, 4, 512)    2048        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_112 (LeakyReLU)     (None, 4, 4, 512)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 2, 2, 512)    4194816     leaky_re_lu_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 2, 2, 512)    2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_113 (LeakyReLU)     (None, 2, 2, 512)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_41 (Conv2DTran (None, 4, 4, 512)    4194816     leaky_re_lu_113[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 4, 4, 512)    2048        conv2d_transpose_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_114 (LeakyReLU)     (None, 4, 4, 512)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 4, 4, 512)    0           leaky_re_lu_114[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 4, 4, 1024)   0           dropout_36[0][0]                 \n",
      "                                                                 leaky_re_lu_112[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_42 (Conv2DTran (None, 8, 8, 512)    8389120     concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 8, 8, 512)    2048        conv2d_transpose_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_115 (LeakyReLU)     (None, 8, 8, 512)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8, 8, 512)    0           leaky_re_lu_115[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 8, 8, 1024)   0           dropout_37[0][0]                 \n",
      "                                                                 leaky_re_lu_111[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_43 (Conv2DTran (None, 16, 16, 512)  8389120     concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 16, 512)  2048        conv2d_transpose_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_116 (LeakyReLU)     (None, 16, 16, 512)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 16, 16, 512)  0           leaky_re_lu_116[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 16, 16, 1024) 0           dropout_38[0][0]                 \n",
      "                                                                 leaky_re_lu_110[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_44 (Conv2DTran (None, 32, 32, 512)  8389120     concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 32, 32, 512)  2048        conv2d_transpose_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_117 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 32, 32, 512)  0           leaky_re_lu_117[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 32, 32, 1024) 0           dropout_39[0][0]                 \n",
      "                                                                 leaky_re_lu_109[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_45 (Conv2DTran (None, 64, 64, 256)  4194560     concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 64, 64, 256)  1024        conv2d_transpose_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_118 (LeakyReLU)     (None, 64, 64, 256)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 64, 64, 256)  0           leaky_re_lu_118[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 64, 64, 512)  0           dropout_40[0][0]                 \n",
      "                                                                 leaky_re_lu_108[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_46 (Conv2DTran (None, 128, 128, 128 1048704     concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 128, 128, 128 512         conv2d_transpose_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_119 (LeakyReLU)     (None, 128, 128, 128 0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 128, 128, 128 0           leaky_re_lu_119[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 128, 128, 256 0           dropout_41[0][0]                 \n",
      "                                                                 leaky_re_lu_107[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_47 (Conv2DTran (None, 256, 256, 64) 262208      concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 256, 256, 64) 256         conv2d_transpose_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_120 (LeakyReLU)     (None, 256, 256, 64) 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 256, 256, 64) 0           leaky_re_lu_120[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 256, 256, 128 0           dropout_42[0][0]                 \n",
      "                                                                 leaky_re_lu_106[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_48 (Conv2DTran (None, 512, 512, 3)  6147        concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 512, 512, 3)  0           conv2d_transpose_48[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 55,994,435\n",
      "Trainable params: 55,983,427\n",
      "Non-trainable params: 11,008\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator Summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 512, 512, 13) 0           input_35[0][0]                   \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 256, 256, 64) 13376       concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_121 (LeakyReLU)     (None, 256, 256, 64) 0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 128, 128, 128 131200      leaky_re_lu_121[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 128, 128, 128 512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_122 (LeakyReLU)     (None, 128, 128, 128 0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 64, 64, 256)  524544      leaky_re_lu_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 64, 64, 256)  1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_123 (LeakyReLU)     (None, 64, 64, 256)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 512)  2097664     leaky_re_lu_123[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 32, 32, 512)  2048        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_124 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 512)  4194816     leaky_re_lu_124[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 32, 32, 512)  2048        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_125 (LeakyReLU)     (None, 32, 32, 512)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 256)  2097408     leaky_re_lu_125[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 32, 32, 256)  1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_126 (LeakyReLU)     (None, 32, 32, 256)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 1)    4097        leaky_re_lu_126[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 1)    0           conv2d_90[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,069,761\n",
      "Trainable params: 9,066,433\n",
      "Non-trainable params: 3,328\n",
      "__________________________________________________________________________________________________\n",
      "Complete Model Summary\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 512, 512, 10) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_16 (Model)                (None, 512, 512, 3)  55994435    input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_17 (Model)                (None, 32, 32, 1)    9069761     input_37[0][0]                   \n",
      "                                                                 model_16[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 65,064,196\n",
      "Trainable params: 55,983,427\n",
      "Non-trainable params: 9,080,769\n",
      "__________________________________________________________________________________________________\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen_model = generator((512, 512, 10), (512, 512, 3), 64)\n",
    "\n",
    "print(\"Generator Summary\")\n",
    "gen_model.summary()\n",
    "\n",
    "print(\"Discriminator Summary\")\n",
    "disc_model = discriminator((512, 512, 10), (512, 512, 3), 64)\n",
    "disc_model.summary()\n",
    "\n",
    "print(\"Complete Model Summary\")\n",
    "cgan_model = cGAN_Model(gen_model, disc_model)\n",
    "cgan_model.summary()\n",
    "\n",
    "gen_model.compile(loss = 'mae', optimizer = gen_optimizer)\n",
    "disc_model.compile(loss = 'binary_crossentropy', optimizer = disc_optimizer)\n",
    "cgan_model.compile(loss =[l1_loss, discriminator_on_generator_loss], loss_weights = loss_weights, optimizer = cgan_optimizer)\n",
    "\n",
    "print(cgan_model.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing:\n",
    "X_train_B = np.load('D:/dev/GeoGAN/DataSet/Data/Resized/npy/Shuffled/X_01Norm.npy')\n",
    "X_train_A = np.load('D:/dev/GeoGAN/DataSet/Data/Resized/npy/Shuffled/Y_01Norm.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train function : \n",
    "# 1. Generate images \n",
    "# 2. Train Discriminator as per Conditional GANs \n",
    "# 3. discriminator.trainable = False\n",
    "# 4. Train DCGAN model \n",
    "\n",
    "d_loss = []\n",
    "dc_gan_loss = []\n",
    "dc = 0 \n",
    "d_l = 0\n",
    "\n",
    "def train(epochs, batch_size):\n",
    "    Y_fake = np.zeros((batch_size,32,32,1))\n",
    "    Y_real = np.random.uniform(0.7, 1, (batch_size,32,32,1))\n",
    "    \n",
    "    for num_epochs in range(epochs):\n",
    "        for num_batch in tqdm(range(int(X_train_B.shape[0]/batch_size))):\n",
    "            \n",
    "            X_before = X_train_B[num_batch*batch_size : (num_batch + 1 )*batch_size , : , : , :]            \n",
    "            X_after = X_train_A[num_batch*batch_size : (num_batch + 1 )*batch_size , : , : , : ]\n",
    "            \n",
    "            noise = generate_noise((1, 512, 512, 1))\n",
    "            gen_images = gen_model.predict([X_before, noise])\n",
    "        \n",
    "            disc_model.trainable = True\n",
    "            \n",
    "            #d_loss_real = disc_model.train_on_batch(np.concatenate([X_before, X_after], axis = -1), Y_real)            \n",
    "            #d_loss_fake = disc_model.train_on_batch(np.concatenate([X_before, gen_images], axis = -1), Y_fake)\n",
    "            d_loss_real = disc_model.train_on_batch([X_before, X_after], Y_real)\n",
    "            d_loss_fake = disc_model.train_on_batch([X_before, gen_images], Y_fake)\n",
    "            \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "       \n",
    "            disc_model.trainable = False\n",
    "            for _ in range(2):\n",
    "                dc_gan_loss = cgan_model.train_on_batch([X_before, noise], [X_after, Y_real])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/291 [00:00<?, ?it/s]D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "\n",
      "  0%|▎                                                                               | 1/291 [00:22<1:50:29, 22.86s/it]\n",
      "  1%|▌                                                                               | 2/291 [00:24<1:19:48, 16.57s/it]\n",
      "  1%|▊                                                                                 | 3/291 [00:26<58:22, 12.16s/it]\n",
      "  1%|█▏                                                                                | 4/291 [00:28<43:28,  9.09s/it]\n",
      "  2%|█▍                                                                                | 5/291 [00:30<32:59,  6.92s/it]\n",
      "  2%|█▋                                                                                | 6/291 [00:32<25:41,  5.41s/it]\n",
      "  2%|█▉                                                                                | 7/291 [00:34<20:35,  4.35s/it]\n",
      "  3%|██▎                                                                               | 8/291 [00:36<17:01,  3.61s/it]\n",
      "  3%|██▌                                                                               | 9/291 [00:37<14:31,  3.09s/it]\n",
      "  3%|██▊                                                                              | 10/291 [00:39<12:46,  2.73s/it]\n",
      "  4%|███                                                                              | 11/291 [00:41<11:33,  2.48s/it]\n",
      "  4%|███▎                                                                             | 12/291 [00:43<10:42,  2.30s/it]\n",
      "  4%|███▌                                                                             | 13/291 [00:45<10:04,  2.17s/it]\n",
      "  5%|███▉                                                                             | 14/291 [00:47<09:38,  2.09s/it]\n",
      "  5%|████▏                                                                            | 15/291 [00:49<09:17,  2.02s/it]\n",
      "  5%|████▍                                                                            | 16/291 [00:51<09:02,  1.97s/it]\n",
      "  6%|████▋                                                                            | 17/291 [00:52<08:52,  1.94s/it]\n",
      "  6%|█████                                                                            | 18/291 [00:54<08:44,  1.92s/it]\n",
      "  7%|█████▎                                                                           | 19/291 [00:56<08:38,  1.91s/it]\n",
      "  7%|█████▌                                                                           | 20/291 [00:58<08:34,  1.90s/it]\n",
      "  7%|█████▊                                                                           | 21/291 [01:00<08:33,  1.90s/it]\n",
      "  8%|██████                                                                           | 22/291 [01:02<08:30,  1.90s/it]\n",
      "  8%|██████▍                                                                          | 23/291 [01:04<08:27,  1.89s/it]\n",
      "  8%|██████▋                                                                          | 24/291 [01:06<08:24,  1.89s/it]\n",
      "  9%|██████▉                                                                          | 25/291 [01:08<08:23,  1.89s/it]\n",
      "  9%|███████▏                                                                         | 26/291 [01:09<08:20,  1.89s/it]\n",
      "  9%|███████▌                                                                         | 27/291 [01:11<08:17,  1.88s/it]\n",
      " 10%|███████▊                                                                         | 28/291 [01:13<08:15,  1.88s/it]\n",
      " 10%|████████                                                                         | 29/291 [01:15<08:12,  1.88s/it]\n",
      " 10%|████████▎                                                                        | 30/291 [01:17<08:11,  1.88s/it]\n",
      " 11%|████████▋                                                                        | 31/291 [01:19<08:09,  1.88s/it]\n",
      " 11%|████████▉                                                                        | 32/291 [01:21<08:07,  1.88s/it]\n",
      " 11%|█████████▏                                                                       | 33/291 [01:23<08:04,  1.88s/it]\n",
      " 12%|█████████▍                                                                       | 34/291 [01:24<08:02,  1.88s/it]\n",
      " 12%|█████████▋                                                                       | 35/291 [01:26<08:01,  1.88s/it]\n",
      " 12%|██████████                                                                       | 36/291 [01:28<07:59,  1.88s/it]\n",
      " 13%|██████████▎                                                                      | 37/291 [01:30<07:57,  1.88s/it]\n",
      " 13%|██████████▌                                                                      | 38/291 [01:32<07:54,  1.88s/it]\n",
      " 13%|██████████▊                                                                      | 39/291 [01:34<07:54,  1.88s/it]\n",
      " 14%|███████████▏                                                                     | 40/291 [01:36<07:52,  1.88s/it]\n",
      " 14%|███████████▍                                                                     | 41/291 [01:38<07:50,  1.88s/it]\n",
      " 14%|███████████▋                                                                     | 42/291 [01:40<07:48,  1.88s/it]\n",
      " 15%|███████████▉                                                                     | 43/291 [01:41<07:46,  1.88s/it]\n",
      " 15%|████████████▏                                                                    | 44/291 [01:43<07:47,  1.89s/it]\n",
      " 15%|████████████▌                                                                    | 45/291 [01:45<07:45,  1.89s/it]\n",
      " 16%|████████████▊                                                                    | 46/291 [01:47<07:42,  1.89s/it]\n",
      " 16%|█████████████                                                                    | 47/291 [01:49<07:39,  1.88s/it]\n",
      " 16%|█████████████▎                                                                   | 48/291 [01:51<07:38,  1.89s/it]\n",
      " 17%|█████████████▋                                                                   | 49/291 [01:53<07:37,  1.89s/it]\n",
      " 17%|█████████████▉                                                                   | 50/291 [01:55<07:42,  1.92s/it]\n",
      " 18%|██████████████▏                                                                  | 51/291 [01:57<07:41,  1.92s/it]\n",
      " 18%|██████████████▍                                                                  | 52/291 [01:59<07:37,  1.91s/it]\n",
      " 18%|██████████████▊                                                                  | 53/291 [02:00<07:33,  1.90s/it]\n",
      " 19%|███████████████                                                                  | 54/291 [02:02<07:30,  1.90s/it]\n",
      " 19%|███████████████▎                                                                 | 55/291 [02:04<07:28,  1.90s/it]\n",
      " 19%|███████████████▌                                                                 | 56/291 [02:06<07:25,  1.90s/it]\n",
      " 20%|███████████████▊                                                                 | 57/291 [02:08<07:23,  1.89s/it]\n",
      " 20%|████████████████▏                                                                | 58/291 [02:10<07:23,  1.90s/it]\n",
      " 20%|████████████████▍                                                                | 59/291 [02:12<07:21,  1.90s/it]\n",
      " 21%|████████████████▋                                                                | 60/291 [02:14<07:19,  1.90s/it]\n",
      " 21%|████████████████▉                                                                | 61/291 [02:16<07:17,  1.90s/it]\n",
      " 21%|█████████████████▎                                                               | 62/291 [02:18<07:14,  1.90s/it]\n",
      " 22%|█████████████████▌                                                               | 63/291 [02:19<07:12,  1.90s/it]\n",
      " 22%|█████████████████▊                                                               | 64/291 [02:21<07:10,  1.90s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████                                                               | 65/291 [02:23<07:08,  1.90s/it]\n",
      " 23%|██████████████████▎                                                              | 66/291 [02:25<07:06,  1.89s/it]\n",
      " 23%|██████████████████▋                                                              | 67/291 [02:27<07:03,  1.89s/it]\n",
      " 23%|██████████████████▉                                                              | 68/291 [02:29<07:01,  1.89s/it]\n",
      " 24%|███████████████████▏                                                             | 69/291 [02:31<06:58,  1.89s/it]\n",
      " 24%|███████████████████▍                                                             | 70/291 [02:33<06:57,  1.89s/it]\n",
      " 24%|███████████████████▊                                                             | 71/291 [02:35<06:55,  1.89s/it]\n",
      " 25%|████████████████████                                                             | 72/291 [02:36<06:53,  1.89s/it]\n",
      " 25%|████████████████████▎                                                            | 73/291 [02:38<06:53,  1.89s/it]\n",
      " 25%|████████████████████▌                                                            | 74/291 [02:40<06:50,  1.89s/it]\n",
      " 26%|████████████████████▉                                                            | 75/291 [02:42<06:49,  1.89s/it]\n",
      " 26%|█████████████████████▏                                                           | 76/291 [02:44<06:46,  1.89s/it]\n",
      " 26%|█████████████████████▍                                                           | 77/291 [02:46<06:44,  1.89s/it]\n",
      " 27%|█████████████████████▋                                                           | 78/291 [02:48<06:42,  1.89s/it]\n",
      " 27%|█████████████████████▉                                                           | 79/291 [02:50<06:41,  1.89s/it]\n",
      " 27%|██████████████████████▎                                                          | 80/291 [02:52<06:39,  1.89s/it]\n",
      " 28%|██████████████████████▌                                                          | 81/291 [02:53<06:36,  1.89s/it]\n",
      " 28%|██████████████████████▊                                                          | 82/291 [02:55<06:36,  1.90s/it]\n",
      " 29%|███████████████████████                                                          | 83/291 [02:57<06:34,  1.89s/it]\n",
      " 29%|███████████████████████▍                                                         | 84/291 [02:59<06:43,  1.95s/it]\n",
      " 29%|███████████████████████▋                                                         | 85/291 [03:02<07:02,  2.05s/it]\n",
      " 30%|███████████████████████▉                                                         | 86/291 [03:04<06:51,  2.01s/it]\n",
      " 30%|████████████████████████▏                                                        | 87/291 [03:05<06:44,  1.98s/it]\n",
      " 30%|████████████████████████▍                                                        | 88/291 [03:07<06:37,  1.96s/it]\n",
      " 31%|████████████████████████▊                                                        | 89/291 [03:09<06:31,  1.94s/it]\n",
      " 31%|█████████████████████████                                                        | 90/291 [03:11<06:26,  1.92s/it]\n",
      " 31%|█████████████████████████▎                                                       | 91/291 [03:13<06:22,  1.91s/it]\n",
      " 32%|█████████████████████████▌                                                       | 92/291 [03:15<06:20,  1.91s/it]\n",
      " 32%|█████████████████████████▉                                                       | 93/291 [03:17<06:17,  1.91s/it]\n",
      " 32%|██████████████████████████▏                                                      | 94/291 [03:19<06:13,  1.90s/it]\n",
      " 33%|██████████████████████████▍                                                      | 95/291 [03:21<06:10,  1.89s/it]\n",
      " 33%|██████████████████████████▋                                                      | 96/291 [03:22<06:08,  1.89s/it]\n",
      " 33%|███████████████████████████                                                      | 97/291 [03:24<06:05,  1.89s/it]\n",
      " 34%|███████████████████████████▎                                                     | 98/291 [03:26<06:03,  1.89s/it]\n",
      " 34%|███████████████████████████▌                                                     | 99/291 [03:28<06:01,  1.89s/it]\n",
      " 34%|███████████████████████████▍                                                    | 100/291 [03:30<06:00,  1.89s/it]\n",
      " 35%|███████████████████████████▊                                                    | 101/291 [03:32<05:59,  1.89s/it]\n",
      " 35%|████████████████████████████                                                    | 102/291 [03:34<05:57,  1.89s/it]\n",
      " 35%|████████████████████████████▎                                                   | 103/291 [03:36<05:54,  1.89s/it]\n",
      " 36%|████████████████████████████▌                                                   | 104/291 [03:38<05:53,  1.89s/it]\n",
      " 36%|████████████████████████████▊                                                   | 105/291 [03:39<05:50,  1.89s/it]\n",
      " 36%|█████████████████████████████▏                                                  | 106/291 [03:41<05:48,  1.89s/it]\n",
      " 37%|█████████████████████████████▍                                                  | 107/291 [03:43<05:47,  1.89s/it]\n",
      " 37%|█████████████████████████████▋                                                  | 108/291 [03:45<05:44,  1.88s/it]\n",
      " 37%|█████████████████████████████▉                                                  | 109/291 [03:47<05:43,  1.89s/it]\n",
      " 38%|██████████████████████████████▏                                                 | 110/291 [03:49<05:40,  1.88s/it]\n",
      " 38%|██████████████████████████████▌                                                 | 111/291 [03:51<05:37,  1.88s/it]\n",
      " 38%|██████████████████████████████▊                                                 | 112/291 [03:53<05:35,  1.88s/it]\n",
      " 39%|███████████████████████████████                                                 | 113/291 [03:54<05:34,  1.88s/it]\n",
      " 39%|███████████████████████████████▎                                                | 114/291 [03:56<05:33,  1.88s/it]\n",
      " 40%|███████████████████████████████▌                                                | 115/291 [03:58<05:31,  1.89s/it]\n",
      " 40%|███████████████████████████████▉                                                | 116/291 [04:00<05:29,  1.89s/it]\n",
      " 40%|████████████████████████████████▏                                               | 117/291 [04:02<05:27,  1.88s/it]\n",
      " 41%|████████████████████████████████▍                                               | 118/291 [04:04<05:25,  1.88s/it]\n",
      " 41%|████████████████████████████████▋                                               | 119/291 [04:06<05:23,  1.88s/it]\n",
      " 41%|████████████████████████████████▉                                               | 120/291 [04:08<05:21,  1.88s/it]\n",
      " 42%|█████████████████████████████████▎                                              | 121/291 [04:10<05:19,  1.88s/it]\n",
      " 42%|█████████████████████████████████▌                                              | 122/291 [04:11<05:17,  1.88s/it]\n",
      " 42%|█████████████████████████████████▊                                              | 123/291 [04:13<05:15,  1.88s/it]\n",
      " 43%|██████████████████████████████████                                              | 124/291 [04:15<05:13,  1.88s/it]\n",
      " 43%|██████████████████████████████████▎                                             | 125/291 [04:17<05:11,  1.88s/it]\n",
      " 43%|██████████████████████████████████▋                                             | 126/291 [04:19<05:10,  1.88s/it]\n",
      " 44%|██████████████████████████████████▉                                             | 127/291 [04:21<05:08,  1.88s/it]\n",
      " 44%|███████████████████████████████████▏                                            | 128/291 [04:23<05:06,  1.88s/it]\n",
      " 44%|███████████████████████████████████▍                                            | 129/291 [04:25<05:04,  1.88s/it]\n",
      " 45%|███████████████████████████████████▋                                            | 130/291 [04:26<05:02,  1.88s/it]\n",
      " 45%|████████████████████████████████████                                            | 131/291 [04:28<05:00,  1.88s/it]\n",
      " 45%|████████████████████████████████████▎                                           | 132/291 [04:30<04:58,  1.88s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████▌                                           | 133/291 [04:32<04:56,  1.88s/it]\n",
      " 46%|████████████████████████████████████▊                                           | 134/291 [04:34<04:54,  1.88s/it]\n",
      " 46%|█████████████████████████████████████                                           | 135/291 [04:36<04:52,  1.88s/it]\n",
      " 47%|█████████████████████████████████████▍                                          | 136/291 [04:38<04:51,  1.88s/it]\n",
      " 47%|█████████████████████████████████████▋                                          | 137/291 [04:40<04:49,  1.88s/it]\n",
      " 47%|█████████████████████████████████████▉                                          | 138/291 [04:41<04:47,  1.88s/it]\n",
      " 48%|██████████████████████████████████████▏                                         | 139/291 [04:43<04:46,  1.89s/it]\n",
      " 48%|██████████████████████████████████████▍                                         | 140/291 [04:45<04:45,  1.89s/it]\n",
      " 48%|██████████████████████████████████████▊                                         | 141/291 [04:47<04:44,  1.89s/it]\n",
      " 49%|███████████████████████████████████████                                         | 142/291 [04:49<04:42,  1.89s/it]\n",
      " 49%|███████████████████████████████████████▎                                        | 143/291 [04:51<04:40,  1.90s/it]\n",
      " 49%|███████████████████████████████████████▌                                        | 144/291 [04:53<04:39,  1.90s/it]\n",
      " 50%|███████████████████████████████████████▊                                        | 145/291 [04:55<04:36,  1.90s/it]\n",
      " 50%|████████████████████████████████████████▏                                       | 146/291 [04:57<04:35,  1.90s/it]\n",
      " 51%|████████████████████████████████████████▍                                       | 147/291 [04:59<04:32,  1.89s/it]\n",
      " 51%|████████████████████████████████████████▋                                       | 148/291 [05:00<04:29,  1.88s/it]\n",
      " 51%|████████████████████████████████████████▉                                       | 149/291 [05:02<04:26,  1.88s/it]\n",
      " 52%|█████████████████████████████████████████▏                                      | 150/291 [05:04<04:25,  1.88s/it]\n",
      " 52%|█████████████████████████████████████████▌                                      | 151/291 [05:06<04:23,  1.88s/it]\n",
      " 52%|█████████████████████████████████████████▊                                      | 152/291 [05:08<04:21,  1.88s/it]\n",
      " 53%|██████████████████████████████████████████                                      | 153/291 [05:10<04:20,  1.88s/it]\n",
      " 53%|██████████████████████████████████████████▎                                     | 154/291 [05:12<04:18,  1.89s/it]\n",
      " 53%|██████████████████████████████████████████▌                                     | 155/291 [05:14<04:16,  1.88s/it]\n",
      " 54%|██████████████████████████████████████████▉                                     | 156/291 [05:16<04:15,  1.89s/it]\n",
      " 54%|███████████████████████████████████████████▏                                    | 157/291 [05:17<04:13,  1.89s/it]\n",
      " 54%|███████████████████████████████████████████▍                                    | 158/291 [05:19<04:11,  1.89s/it]\n",
      " 55%|███████████████████████████████████████████▋                                    | 159/291 [05:21<04:09,  1.89s/it]\n",
      " 55%|███████████████████████████████████████████▉                                    | 160/291 [05:23<04:07,  1.89s/it]\n",
      " 55%|████████████████████████████████████████████▎                                   | 161/291 [05:25<04:06,  1.89s/it]\n",
      " 56%|████████████████████████████████████████████▌                                   | 162/291 [05:27<04:05,  1.90s/it]\n",
      " 56%|████████████████████████████████████████████▊                                   | 163/291 [05:29<04:02,  1.89s/it]\n",
      " 56%|█████████████████████████████████████████████                                   | 164/291 [05:31<04:00,  1.89s/it]\n",
      " 57%|█████████████████████████████████████████████▎                                  | 165/291 [05:33<03:58,  1.89s/it]\n",
      " 57%|█████████████████████████████████████████████▋                                  | 166/291 [05:34<03:56,  1.89s/it]\n",
      " 57%|█████████████████████████████████████████████▉                                  | 167/291 [05:36<03:54,  1.89s/it]\n",
      " 58%|██████████████████████████████████████████████▏                                 | 168/291 [05:38<03:51,  1.89s/it]\n",
      " 58%|██████████████████████████████████████████████▍                                 | 169/291 [05:40<03:50,  1.89s/it]\n",
      " 58%|██████████████████████████████████████████████▋                                 | 170/291 [05:42<03:48,  1.89s/it]\n",
      " 59%|███████████████████████████████████████████████                                 | 171/291 [05:44<03:46,  1.89s/it]\n",
      " 59%|███████████████████████████████████████████████▎                                | 172/291 [05:46<03:44,  1.89s/it]\n",
      " 59%|███████████████████████████████████████████████▌                                | 173/291 [05:48<03:42,  1.89s/it]\n",
      " 60%|███████████████████████████████████████████████▊                                | 174/291 [05:50<03:40,  1.89s/it]\n",
      " 60%|████████████████████████████████████████████████                                | 175/291 [05:51<03:39,  1.89s/it]\n",
      " 60%|████████████████████████████████████████████████▍                               | 176/291 [05:53<03:37,  1.89s/it]\n",
      " 61%|████████████████████████████████████████████████▋                               | 177/291 [05:55<03:35,  1.89s/it]\n",
      " 61%|████████████████████████████████████████████████▉                               | 178/291 [05:57<03:33,  1.89s/it]\n",
      " 62%|█████████████████████████████████████████████████▏                              | 179/291 [05:59<03:31,  1.89s/it]\n",
      " 62%|█████████████████████████████████████████████████▍                              | 180/291 [06:01<03:29,  1.89s/it]\n",
      " 62%|█████████████████████████████████████████████████▊                              | 181/291 [06:03<03:27,  1.89s/it]\n",
      " 63%|██████████████████████████████████████████████████                              | 182/291 [06:05<03:26,  1.89s/it]\n",
      " 63%|██████████████████████████████████████████████████▎                             | 183/291 [06:07<03:24,  1.89s/it]\n",
      " 63%|██████████████████████████████████████████████████▌                             | 184/291 [06:08<03:22,  1.89s/it]\n",
      " 64%|██████████████████████████████████████████████████▊                             | 185/291 [06:10<03:20,  1.89s/it]\n",
      " 64%|███████████████████████████████████████████████████▏                            | 186/291 [06:12<03:18,  1.89s/it]\n",
      " 64%|███████████████████████████████████████████████████▍                            | 187/291 [06:14<03:16,  1.89s/it]\n",
      " 65%|███████████████████████████████████████████████████▋                            | 188/291 [06:16<03:14,  1.89s/it]\n",
      " 65%|███████████████████████████████████████████████████▉                            | 189/291 [06:18<03:12,  1.89s/it]\n",
      " 65%|████████████████████████████████████████████████████▏                           | 190/291 [06:20<03:11,  1.89s/it]\n",
      " 66%|████████████████████████████████████████████████████▌                           | 191/291 [06:22<03:10,  1.90s/it]\n",
      " 66%|████████████████████████████████████████████████████▊                           | 192/291 [06:24<03:07,  1.90s/it]\n",
      " 66%|█████████████████████████████████████████████████████                           | 193/291 [06:26<03:06,  1.90s/it]\n",
      " 67%|█████████████████████████████████████████████████████▎                          | 194/291 [06:27<03:04,  1.90s/it]\n",
      " 67%|█████████████████████████████████████████████████████▌                          | 195/291 [06:29<03:02,  1.90s/it]\n",
      " 67%|█████████████████████████████████████████████████████▉                          | 196/291 [06:31<03:00,  1.90s/it]\n",
      " 68%|██████████████████████████████████████████████████████▏                         | 197/291 [06:33<02:58,  1.90s/it]\n",
      " 68%|██████████████████████████████████████████████████████▍                         | 198/291 [06:35<02:57,  1.90s/it]\n",
      " 68%|██████████████████████████████████████████████████████▋                         | 199/291 [06:37<02:54,  1.90s/it]\n",
      " 69%|██████████████████████████████████████████████████████▉                         | 200/291 [06:39<02:52,  1.90s/it]\n",
      " 69%|███████████████████████████████████████████████████████▎                        | 201/291 [06:41<02:50,  1.90s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-e91c35a043c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-5591f7996db1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mdisc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mdc_gan_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_real\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-76c5c541bdd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Save weights of generator and discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpath_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'60_epochs_gen.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpath_disc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weights'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'60_epochs_disc.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgen_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_disc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Save weights of generator and discriminator \n",
    "path_gen = os.path.join('weights','60_epochs_gen.h5')\n",
    "path_disc = os.path.join('weights','60_epochs_disc.h5')\n",
    "gen_model.save_weights(path_gen)\n",
    "disc_model.save_weights(path_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights of generator and discriminator \n",
    "path_gen = os.path.join('train_weights_p2p','150_epochs_gen.h5py')\n",
    "#path_disc = os.path.join('train_weights_p2p','150_epochs_disc.h5')\n",
    "gen_model.load_weights(path_gen)\n",
    "#disc_model.load_weights(path_disc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
